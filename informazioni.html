<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Informazioni - Progetto Libri</title>
    <link rel="stylesheet" href="informazioni.css">
</head>
<body>
    <header>
        <nav>
            <ul class="navbar">
                <li><a href="index.html">Home</a></li>
                <li><a href="informazioni.html">Informazioni sul Progetto</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section class="intro">
            <h1>Informazioni sul Progetto</h1>
            <p>Inserire qui l'abstract del progetto.</p>
        </section>

        <section class="indice">
            <h2>Indice</h2>
            <ol>
                <li><a href="#introduzione">Introduzione</a></li>
                <li>
                    <a href="#creazione-dataset">Creazione Dataset</a>
                    <ol>
                        <li><a href="#web-scraping">Web Scraping su Open Library</a></li>
                        <li>
                            <a href="#conversione-correzzione">Conversione in txt e correzione dei testi</a>
                        </li>
                        <li><a href="#gutenberg">Gutenberg</a></li>
                        <li><a href="#estrazione-frasi">Estrazione Frasi Casuali</a></li>
                        <li><a href="#parole-frequenti">Parole più Frequenti</a></li>
                        <li><a href="#dataset">Dataset</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#similarita-semantica">Similarità Semantica</a>
                    <ol>
                        <li><a href="#word-net">Similarità tra Parole con Word Net</a></li>
                        <li>
                            <a href="#similarita-frasi">Similarità tra Frasi</a>
                            <ol>
                                <li><a href="#bert">BERT</a></li>
                                <li><a href="#framenet">FrameNet</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#confronti-risultati">Confronti e Risultati</a></li>
                <li><a href="#sito-web">Sito Web</a></li>
                <li><a href="#conclusioni">Conclusioni</a></li>
                <li><a href="#references">References</a></li>
            </ol>
        </section>

        <section id="introduzione">
            <h2>Introduzione</h2>
            <p>Il progetto nasce dall’intento di consolidare e ampliare le conoscenze acquisite durante le lezioni,
                mettendo in pratica quanto appreso. L’obiettivo è
                stato quello di approfondire e accrescere le competenze personali per migliorare la padronanza degli
                strumenti acquisiti e progredire verso un approccio di lavoro piu autonomo e consapevole. Nel corso dello sviluppo, sono stati approfonditi anche altri aspetti legati alla programmazione, arricchendo ulteriormente il bagaglio di competenze.
                Per perseguire questi obiettivi, si e scelto di orientare il progetto verso interessi personali legati
                alla letteratura e all’ambito dell’elaborazione del
                linguaggio naturale (NLP), con particolare attenzione alla similarita semantica. Il progetto si presenta come una proposta alternativa per consigliare libri da leggere, andando oltre criteri
                tradizionali come genere e autore, basandosi invece sulla similarita semantica tra i contenuti. Lo sviluppo del progetto puo essere diviso in tre parti: `
                costruzione di un dataset, elaborazione di codice
                per l’analisi semantica e creazione di un sito web.</p>
        </section>

        <section id="creazione-dataset">
            <h2>Creazione Dataset</h2>
            <p>Per condurre le analisi di similarita semantica e sviluppare un sito affidabile, la prima fase e consistita nella creazione di un dataset. Abbiamo
                scelto di raccogliere un insieme di libri digitali
                da fonti affidabili, come Open Library e Gutenberg Library, per estrarre frasi casuali e parole frequenti, arricchendo i dati con informazioni strutturate, tra cui titolo, autore, anno di pubblicazione,
                genere e immagine di copertina.</p>

            <section id="web-scraping">
                <h3>Web Scraping su Open Library</h3>
                <p>Abbiamo utilizzato la piattaforma Open Library
                    per raccogliere una vasta gamma di libri in formato PDF disponibili gratuitamente. Abbiamo
                    sviluppato un codice Python, basato su BeautifulSoup e Requests, per automatizzare il processo di
                    navigazione, analisi del codice HTML, download
                    dei file PDF e salvataggio in una directory dedicata, con ogni file nominato in base al titolo del
                    libro
                    Durante questo processo, il codice e stato progettato per gestire eventuali errori, come file mancanti
                    o problemi di rete, garantendo così un’esecuzione
                    robusta. Inoltre, e stato inserito un ritardo di due secondi tra una richiesta e l’altra per rispettare
                    i limiti del server e prevenire il blocco del programma.</p>
            </section>

            <section id="conversione-correzzione">
                <h3>Conversione in txt e Correzione dei Testi</h3>
                <p>Dopo aver completato la raccolta dei libri in formato PDF, il passo successivo e stato estrarre il contenuto testuale dai file per renderlo utilizzabile
                    nelle successive analisi. A tale scopo, abbiamo
                    scritto un codice Python che si avvale della libreria PyPDF2, progettato per leggere un file PDF e
                    restituirne il testo completo; per ciascuna pagina,
                    il testo viene estratto e aggiunto a una stringa che,
                    alla fine del processo, contiene il contenuto testuale integrale del libro. Questo approccio e stato scelto per la sua semplicita ed efficienza nel gestire 
                    file PDF di diverse dimensioni e strutture. Tuttavia, in alcuni casi, l’estrazione del testo puo essere influenzata dalla qualita della scansione del `
                    PDF o dal formato del documento stesso, come
                    nel caso di file composti da immagini invece che
                    da testo selezionabile. Per questo, dopo aver estratto il contenuto dai PDF, abbiamo corretto e normalizzato il testo. Per questa fase, abbiamo optato per un approccio basato su textblob e sulla
                    libreria wordninja. Il codice sviluppato esegue
                    la normalizzazione in piu passaggi. Per prima cosa, sfrutta textblob per correggere in automatico
                    i testi, dopodiche utilizza le espressioni regolari per eliminare le interruzioni di linea e gli spazi
                    consecutivi. Successivamente, si avvale di wordninja per dividere parole unite in modo errato, un
                    problema comune nei testi derivati da estrazioni
                    PDF. Una volta normalizzato, il testo viene salvato
                    in un nuovo file.
                    </p>
            </section>

            <section id="gutenberg">
                <h3>Gutenberg</h3>
                <p>Per l’estrazione di testi dal corpus di Gutenberg
                    di NLTK, e stato sviluppato un processo che recupera contenuti testuali e organizza i metadati in
                    un file JSON in modo sistematico. Il corpus viene
                    scaricato attraverso NLTK e i dati archiviati in una
                    directory dedicata. Per ogni libro vengono estratti
                    il testo completo, il titolo e l’autore (o ”Sconosciuto” se mancante) e salvati in file di testo col
                    nome originale del file Gutenberg. I metadati vengono raccolti in un dizionario Python e esportati
                    in JSON. Questo approccio e stato esteso ai libri di Open Books, arricchendo con ulteriori informazioni come genere e immagini di copertina, in
                    modo da conservare i dati completi di tutti i libri
                    utilizzati.</p>
            </section>

            <section id="estrazione-frasi">
                <h3>Estrazione Frasi Casuali</h3>
                <p>Una volta ottenuti i file in formato txt corretti
                    e completi, e seguita l’estrazione di frasi casuali dai libri. Poiche i metodi di analisi semantica non funzionano su segmenti di testo piu lunghi, `
                    si e scelto di selezionare 30 frasi per ciascun libro. Questa fase ha comportato la creazione di
                    un algoritmo in grado di individuare frasi complete, evitando sequenze di testo troppo brevi o
                    che potessero risultare non pertinenti. Il testo è stato ripulito da caratteri speciali come i simboli
                    di backslash, barra, asterisco e accento circostante;
                    trattando anche i casi delle abbreviazioni, che non
                    indicano la fine di una frase per evitare che il programma trattasse erroneamente questi casi come
                    conclusioni di frasi. Per isolare correttamente le
                    frasi, e stato utilizzato un pattern di espressioni regolari che prendeva in considerazione i segni di
                    punteggiatura, implementando poi un filtro che escludeva le frasi troppo brevi, quelle scritte interamente in maiuscolo o quelle che contenevano piu`
                    di una sequenza consecutiva di lettere maiuscole,
                    in quanto ritenute non idonee per il campione.
                    Una volta ottenute le frasi, il programma selezionava casualmente 30 frasi, assicurandosi che queste
                    rispettassero i criteri di lunghezza e formato. Ogni
                    frase e stata controllata manualmente per correggere eventuali errori e garantire che fosse completamente leggibile e corretta dal punto di vista
                    sintattico. Il numero di frasi che abbiamo scelto
                    di estrarre rappresenta un compromesso tra la capacita di calcolo dei nostri software e la necessità
                    di garantire la massima affidabilita possibile nei risultati. Tuttavia, suggeriamo di aumentare il numero di frasi estratte se possibile, poiche un corpus più ampio migliora la probabilit ` a che il campione `
                    sia più rappresentativo. </p>
            </section>

            <section id="parole-frequenti">
                <h3>Parole più Frequenti</h3>
                <p>Nel processo di estrazione delle parole frequenti,
                    il codice sviluppato si occupa di leggere e processare il testo da ciascun file per estrarre un insieme
                    di 50 parole rilevanti in base alla loro frequenza.
                    L’obiettivo e ottenere un elenco delle parole più
                    comuni all’interno di ciascun libro, mentre si effettuano diverse operazioni di pulizia e filtraggio per
                    garantire che solo le parole significative vengano
                    considerate. Il testo, dopo essere stato normalizzato con anche la rimozione della punteggiatura,
                    viene quindi suddiviso in singole parole, applicando un filtro per rimuovere le stopwords. Inoltre, viene impiegata una lista di parole personalizzate da escludere, comprendente termini che, pur
                    essendo frequenti nel contesto del testo, non hanno
                    valore informativo ai fini dell’analisi, come nomi
                    propri e termini troppo specifici. Solo le parole
                    con almeno tre lettere e che non contengano caratteri tipografici particolari come apici o virgolette
                    vengono mantenute. Infine, una volta che tutte le
                    parole rilevanti sono state selezionate, viene calcolata la loro frequenza d’occorrenza nel testo, e
                    viene restituito un elenco delle parole piu comuni, ordinate per frequenza.</p>
            </section>

            <section id="dataset">
                <h3>Dataset</h3>
                <p>Il dataset finale, che verra utilizzato nel calcolo della similarita e nel sito, si compone delle 30 frasi casuali estratte da ogni libro, delle 50 parole piu`
                    frequenti e delle informazioni sui libri. Queste informazioni sono state estratte dai metadati di ogni
                    libro e sono state ordinate in un DataFrame creato
                    utilizzando la libreria pandas e in un file JSON,
                    utile nelle dinamiche del sito internet. Questo file
                    include il titolo, l’autore, l’anno di pubblicazione,
                    il genere e il percorso dell’immagine di copertina
                    del libro.</p>
            </section>
        </section>

        <section id="similarita-semantica">
            <h2>Similarità Semantica</h2>
            <p>La seconda fase e iniziata con una ricerca sulla letteratura relativa alla similarita semantica, la quale puo essere definita come la distanza tra i significati semantici di una coppia di parole, frasi, paragrafi o documenti. Dopo una serie di ricerche si
                e deciso di concentrarsi sull’analisi di coppie di parole e frasi. Inoltre, per quanto riguarda la similarita tra frasi, bisogna tenere presente che non esiste un quadro teorico a livello frasale che stabilisca come determinare se due frasi sono semanticamente equivalenti, i criteri sono spesso soggettivi.</p>

            <section id="word-net">
                <h3>Similarità tra Parole con Word Net</h3>
                <p>Per l’analisi delle coppie di parole si e scelto di 
                    utilizzare WordNet1
                    , un enorme database che raggruppa nomi, verbi, aggettivi e avverbi in cognitive synonyms, insiemi di sinonimi che per il
                    parlante nativo indicano un concetto specifico. I
                    synset sono collegati tra loro attraverso relazioni
                    concettuali-semantiche e lessicali, come la relazione di iperonimia, la quale e stata scelta per 
                    il calcolo dato che e la relazione codificata con 
                    maggiore frequenza su WordNet. Inoltre, i synset
                    contengono una breve definizione, gloss, la quale
                    comprende anche una o piu frasi brevi con esempi 
                    d’uso. Per il calcolo della similarita si è deciso di 
                    basarsi su questi due aspetti, unendo dunque:</p>
                    <ul>
                        <li>La similarita basata sui percorsi, path-based, 
                            la quale sfrutta la gerarchia concettuale di
                            WordNet, dove valori piu alti indicano relazioni semantiche piu strette.</li>
                        <li> La similarita basata sulle gloss, la quale utilizza le definizioni e gli esempi dei synset, aggregandoli in un gloss esteso, che viene successivamente preprocessato per rimuovere
                            stopword e punteggiatura, seguito dalla lemmatizzazione.Idea e molto semplice: se due 
                            parole sono semanticamente simili, allora le
                            glosse estese avranno piu parole in comune. 
                            La similarita tra gloss estesi viene valutata utilizzando il coefficiente di similarita di Jac
                            card, concentrandosi su contenuti comuni tra
                            insiemi.</li>
                    </ul>
                <p>Il punteggio finale le combina, garantendo che
                    vengano sfruttate sia la gerarchia che le glosse presenti in WordNet. Data l’idea di confrontare le
                    parole piu frequenti, abbiamo adattato il codice 
                    rendendolo in grado di confrontare liste di parole. Infine, il codice e stato testato con il dataset 
                    WordSim353, ottenendo punteggi di correlazione di Pearson (0.5748) e Spearman (0.6389).
                    Questi punteggi indicano che il metodo utilizzato
                    è abbastanza efficiente, ma non è perfetto; a confronto alcuni sistemi allo stato dell’arte raggiungono un coefficiente di Spearman attorno a 0.8.</p>
                </p>
            </section>

            <section id="similarita-frasi">
                <h3>Similarità tra Frasi</h3>
                <p>Per l’analisi delle frasi intere si e scelto di utilizzare due approcci diversi, uno più tradizionale e il secondo più innovativo.</p>

                <section id="bert">
                    <h4>BERT</h4>
                    <p>Il primo sistema di calcolo della similarita semantica tra frasi e stato implementato utilizzando BERT, in particolare il modello bert-base-uncased,
                        pre-addestrato su un ampio corpus di testi in
                        inglese, della libreria Hugging Face Transformers. Il tokenizzatore di BERT segmenta le frasi
                        in token e li converte in rappresentazioni numeriche compatibili con il modello. Inoltre, il tokenizzatore gestisce automaticamente la lunghezza
                        delle frasi applicando il padding per uniformarle
                        o la troncatura se superano il limite di 128 token. Dopo la tokenizzazione si attua l’estrazione
                        di embedding: il modello genera vettori semantici per ogni frase calcolando la media dei vettori degli ultimi strati. Si e scelto di utilizzare
                        with torch.no_grad(): per ridurre l’uso
                        della memoria e velocizzare il calcolo. Infine, la
                        similarita tra due frasi è determinata confrontando
                        i loro rispettivi embedding tramite la similarità
                        coseno. Il codice e stato poi testato con il dataset 
                        GLUE di STS-benchmark, ottenendo una correlazione di Pearson: 0.87 e una correlazione di
                        spearman: 0.86; sono punteggi buoni, anche se lo
                        stato dell’arte attuale si aggira attorno allo 0.92.</p>
                </section>

                <section id="framenet">
                    <h4>FrameNet</h4>
                    <p>L’ultimo approccio che abbiamo sviluppato si
                        basa vagamente sul lavoro di basile-etal-2018-measuring. Il nostro approccio propone un
                        metodo per calcolare la similarita semantica utilizzando le informazioni combinate di FrameNet e
                        WordNEt, combinando la struttura semantica dai
                        tipi di frame di FrameNet con la ricchezza del
                        database di WordNet. La semantica dei frame
                        e stata sviluppata di Fillmore, mettendo in relazione significato linguistico e conoscenza enciclopedica del mondo. In particolare, un frame è
                        definitivo come una struttura concettuale simile a
                        una scena che descrive un particolare tipo di situazione, oggetto o evento, insieme ai partecipanti
                        e agli elementi necessari per quel frame. Le parole che evocano un frame sono definite come lexical units (LUs), mentre i frame elements sono le
                        componenti chiave che definiscono e contestualizzazione il significato di un evento o di un concetto
                        descritto dal frame, hanno la funzione di un ruolo
                        semantico. Basandosi su questi aspetti, la nostra
                        metodologia si basa su tre componenti principali:
                    <ul>
                        <li>Relatedness tra Frame Types: i frame semantici di FrameNet sono identificati tramite
                            le Lexical Units (LUs) associate alle parole
                            di ciascuna frase. La similarita tra i tipi di `
                            frame e calcolata come la media delle simi- `
                            larita tra tutte le coppie di frame, utilizzando `
                            una soglia per filtrare similarita irrilevanti.</li>
                        <li> Relatedness tra Frame Elements I frame elements di ciascun frame sono analizzati per
                            valutare la loro affinita tramite WordNet.</li>
                        <li>Relatedness complessiva: la misura complessiva combina i due componenti sopra menzionati utilizzando una pesatura dinamica.
                            Ogni frase viene analizzata tramite un processo di tokenizzazione e POS-tagging, per
                            poi estrarre le Lexical Units, frame types e
                            elementi semantici da ciascun token.
                            </li>
                    </ul>
                    Per velocizzare il calcolo e stata inclusa una parte `
                    di precaricamento e parallelizzazione, evitando
                    calcoli ripetitivi e migliorando l’efficienza. Il
                    codice e stato testato con il dataset GLUE di STS- `
                    benchmark su google colab, ottenendo una correlazione di Pearson pari a 0.41 e una correlazione
                    di Spearman pari a 0.39; sono punteggi moderati,
                    mostrando che si possono ottenere dei miglioramenti, magari considerando anche le relazioni gerarchiche presenti in FrameNet.</p>
                </section>
            </section>
        </section>

        <section id="confronti-risultati">
            <h2>Confronti e Risultati</h2>
            <p>Nel processo di analisi e confronto tra i libri, abbiamo utilizzato i tre diversi modelli di similarita`
                con il fine di generare dei file JSON che fossero
                facilmente utilizzabili dal sito per visualizzare e
                interagire con i risultati. L’applicazione del modello di confronto basato sulle parole ha richiesto
                un codice relativamente semplice, che produce un
                file JSON strutturato in cui, per ogni libro, vengono riportati i libri con cui e stato confrontato, `
                insieme ai rispettivi punteggi di similarita. Questo `
                modello, confronta direttamente le liste di parole
                frequenti di due libri, fornendo un unico punteggio
                finale, rappresentativo della similarita complessiva `
                tra i due. Il modello BERT per il confronto tra frasi
                ha richiesto un approccio piu complesso poich ` e´
                fornisce un punteggio di similarita per ogni cop- `
                pia di frasi e ogni libro presenta 30 frasi. Questo
                significa che il risultato del confronto tra due libri
                non viene rappresentato da un singolo valore, ma
                da 900 (30 x 30 frasi = 900 risultati). Inoltre,
                questo tipo di analisi puo generare risultati con un `
                ampio range di valori, a causa della casualita nel `
                campionamento delle frasi per il confronto. Ad
                esempio, una singola frase molto simile a un’altra
                potrebbe generare un punteggio elevato, ma non
                essere rappresentativa della relazione complessiva
                tra i due libri. Per migliorare la validita dei risultati finali, e stata applicata una strategia di calcolo `
                della media ponderata. Questa media tiene conto
                del numero di risultati sopra una certa soglia (nel
                nostro caso, una soglia di similarita pari a 0.15), `
                in modo che la presenza di un numero maggiore
                di frasi simili valga piu dei punteggi individuali di `
                singole coppie di frasi. In altre parole, i punteggi
                finali non dipendono solo dalla similarita di una `
                coppia di frasi, ma anche dalla frequenza con cui
                le frasi di due libri risultano simili tra loro. Questo
                approccio aiuta a ridurre l’impatto della casualita`
                e a ottenere una valutazione piu affidabile e coer- `
                ente della similarita tra i libri. Una volta calco- `
                lata la media ponderata per ciascun confronto di
                frasi, anche questi risultati vengono salvati in un
                file JSON, strutturato in modo da includere, per
                ogni libro, le informazioni relative ai confronti con
                gli altri libri, accompagnate dai punteggi di similarita medi ponderati.</p>
        </section>

        <section id="sito-web">
            <h2>Sito Web</h2>
            <p>Il sito web e stato realizzato con l’obiettivo di `
                consentire agli utenti di individuare libri seman-
                ticamente simili tra di loro. La homepage permette di effettuare una ricerca diretta utilizzando
                una barra dedicata, oppure di esplorare il dataset
                dei libri analizzati, da cui e possibile accedere `
                a informazioni dettagliate sui singoli libri e, selezionandone uno, scoprire i tre titoli piu sim- `
                ili. La similarita tra i libri ` e calcolata utiliz- `
                zando un metodo che combina tre modelli distinti, ciascuno con un peso specifico. Il modello basato sulle parole ha un peso maggiore, seguito da quello basato su BERT, mentre il modello
                basato su FrameNet, risultato meno affidabile, ha
                un’influenza minore sul calcolo finale. Per ogni
                libro simile viene mostrata una percentuale complessiva di similarita, offrendo un’indicazione sin- `
                tetica ma significativa del grado di affinita. In `
                aggiunta, e possibile accedere ai dettagli avanzati `
                per confrontare i risultati forniti dai singoli modelli, evidenziando eventuali differenze nei calcoli
                e nelle interpretazioni.
                Il sito include anche una sezione informativa dedicata al progetto, in cui sono spiegati gli obiettivi
                e le metodologie utilizzate. Dal punto di vista tecnico, il sito e stato sviluppato utilizzando HTML `
                per la struttura, CSS per lo stile e la responsivita,`
                e JavaScript per rendere interattiva la navigazione
                e per gestire dinamicamente i dati caricati.</p>
        </section>

        <section id="conclusioni">
            <h2>Conclusioni</h2>
            <p>Testo delle conclusioni.</p>
        </section>

        <section id="references">
            <h2>References</h2>
            <p>Beautiful Soup documentation, https://www.
                crummy.com/software/BeautifulSoup/
                bs4/doc<br /><br />
                spaCy documentation, https://spacy.io/
                usage/linguistic-features<br /><br />
                Matplotlib documentation, https://
                matplotlib.org/stable/users/index.
                html<br /><br />
                Flask documentation, https://flask.
                palletsprojects.com/en/3.0.x<br /><br />
                Bootrap documentation, https://
                getbootstrap.com/docs/5.3/</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Progetto Similarità Semantica</p>
    </footer>
</body>
</html>
