<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Informazioni - Progetto Libri</title>
    <link rel="stylesheet" href="informazioni.css">
</head>
<body>
    <header>
        <nav>
            <ul class="navbar">
                <li><a href="index.html">Home</a></li>
                <li><a href="informazioni.html">Informazioni sul Progetto</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section class="intro">
            <h1>Informazioni sul Progetto</h1>
            <p>

SemanticShelf è un progetto che esplora nuove modalità di raccomandazione di libri, utilizzando la similarità semantica dei contenuti per superare i limiti dei criteri tradizionali, come genere e autore. L'iniziativa unisce l'elaborazione del linguaggio naturale (NLP) e lo sviluppo web per offrire una piattaforma innovativa e intuitiva.

Il sistema si basa su un dataset progettato per estrarre informazioni significative dai libri e su modelli avanzati di analisi semantica. Questi modelli, che includono approcci tradizionali e basati su reti neurali, valutano le relazioni tra parole e frasi per identificare connessioni profonde tra i testi.

Il risultato finale è un sito web interattivo, progettato per presentare raccomandazioni personalizzate e confronti dettagliati tra i libri. SemanticShelf mira non solo a proporre letture affini agli interessi degli utenti, ma anche a dimostrare il potenziale dell'NLP per innovare il modo in cui accediamo alla conoscenza e alla cultura.</p>
        </section>

        <section class="indice">
            <h2>Indice</h2>
            <ol>
                <li><a href="#introduzione">Introduzione</a></li>
                <li>
                    <a href="#creazione-dataset">Creazione Dataset</a>
                    <ol>
                        <li><a href="#web-scraping">Web Scraping su Open Library</a></li>
                        <li>
                            <a href="#conversione-correzzione">Conversione in txt e correzione dei testi</a>
                        </li>
                        <li><a href="#gutenberg">Gutenberg</a></li>
                        <li><a href="#estrazione-frasi">Estrazione Frasi Casuali</a></li>
                        <li><a href="#parole-frequenti">Parole più Frequenti</a></li>
                        <li><a href="#dataset">Dataset</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#similarita-semantica">Similarità Semantica</a>
                    <ol>
                        <li><a href="#word-net">Similarità tra Parole con Word Net</a></li>
                        <li>
                            <a href="#similarita-frasi">Similarità tra Frasi</a>
                            <ol>
                                <li><a href="#bert">BERT</a></li>
                                <li><a href="#framenet">FrameNet</a></li>
                            </ol>
                        </li>
                    </ol>
                </li>
                <li><a href="#confronti-risultati">Confronti e Risultati</a></li>
                <li><a href="#sito-web">Sito Web</a></li>
                <li><a href="#conclusioni">Conclusioni</a></li>
                <li><a href="#references">References</a></li>
            </ol>
        </section>

        <section id="introduzione">
            <h2>Introduzione</h2>
            <p>Il progetto <b>SemanticShelf</b> rappresenta un’esplorazione delle potenzialità 
                dell’elaborazione del linguaggio naturale (NLP) applicata al settore della raccomandazione 
                di libri. Attraverso un approccio metodico e innovativo, il progetto affronta le sfide legate 
                alla raccolta e organizzazione dei dati, alla valutazione della similarità semantica, e 
                alla presentazione dei risultati tramite un’interfaccia web interattiva.
                In questa pagina descriviamo le fasi fondamentali del lavoro svolto, partendo dalla creazione 
                e pulizia di un dataset strutturato, fino allo sviluppo di algoritmi per analisi avanzate basate 
                su WordNet, BERT e FrameNet. Viene inoltre illustrata la costruzione di questo sito web, progettato per 
                rendere accessibili i risultati agli utenti e favorire un’interazione diretta con il sistema. Ogni sezione 
                evidenzia le scelte progettuali e le tecniche adottate per superare le problematiche emerse, mostrando 
                come ciascun componente del progetto contribuisca al risultato finale.
                SemanticShelf non si limita alla realizzazione di un sistema di raccomandazione: rappresenta un 
                percorso di ricerca e sviluppo che integra strumenti tecnologici e concetti teorici per proporre un approccio 
                innovativo all’analisi semantica dei testi e alla loro valorizzazione.

</p>
        </section>

        <section id="creazione-dataset">
            <h2>Creazione Dataset</h2>
            <p>Per condurre le analisi di similarità semantica e sviluppare un sito affidabile, la prima fase è consistita nella creazione di un dataset. Abbiamo
                scelto di raccogliere un insieme di libri digitali
                da fonti affidabili, come Open Library e Gutenberg Library, per estrarre frasi casuali e parole frequenti, arricchendo i dati con informazioni strutturate, tra cui titolo, autore, anno di pubblicazione,
                genere e immagine di copertina.</p>

            <section id="web-scraping">
                <h3>Web Scraping su Open Library</h3>
                <p>Abbiamo utilizzato la piattaforma Open Library
                    per raccogliere una vasta gamma di libri in formato PDF disponibili gratuitamente. Abbiamo
                    sviluppato un codice Python, basato su BeautifulSoup e Requests, per automatizzare il processo di
                    navigazione, analisi del codice HTML, download
                    dei file PDF e salvataggio in una directory dedicata, con ogni file nominato in base al titolo del
                    libro
                    Durante questo processo, il codice è stato progettato per gestire eventuali errori, come file mancanti
                    o problemi di rete, garantendo così un’esecuzione
                    robusta. Inoltre, è stato inserito un ritardo di due secondi tra una richiesta e l’altra per rispettare
                    i limiti del server e prevenire il blocco del programma.</p>
            </section>

            <section id="conversione-correzzione">
                <h3>Conversione in txt e Correzione dei Testi</h3>
                <p>Dopo aver completato la raccolta dei libri in formato PDF, il passo successivo è stato estrarre il contenuto testuale dai file per renderlo utilizzabile
                    nelle successive analisi. A tale scopo, abbiamo
                    scritto un codice Python che si avvale della libreria Pdfplumber, progettato per leggere un file PDF e
                    restituirne il testo completo; per ciascuna pagina,
                    il testo viene estratto e aggiunto a una stringa che,
                    alla fine del processo, contiene il contenuto testuale integrale del libro. Questo approccio è stato scelto per la sua semplicità ed efficienza nel gestire 
                    file PDF di diverse dimensioni e strutture. Tuttavia, in alcuni casi, l’estrazione del testo puo essere influenzata dalla qualità della scansione del 
                    PDF o dal formato del documento stesso, come
                    nel caso di file composti da immagini invece che
                    da testo selezionabile. Per questo, dopo aver estratto il contenuto dai PDF, abbiamo corretto e normalizzato il testo. Per questa fase, abbiamo optato per un approccio basato su textblob e sulla
                    libreria wordninja. Il codice sviluppato esegue
                    la normalizzazione in piu passaggi. Per prima cosa, sfrutta textblob per correggere in automatico
                    i testi, dopodiché utilizza le espressioni regolari per eliminare le interruzioni di linea e gli spazi
                    consecutivi. Successivamente, si avvale di wordninja per dividere parole unite in modo errato, un
                    problema comune nei testi derivati da estrazioni
                    PDF. Una volta normalizzato, il testo viene salvato
                    in un nuovo file in formato .txt.
                    </p>
            </section>

            <section id="gutenberg">
                <h3>Estrazione libri da Gutenberg</h3>
                <p>Per l’estrazione di testi dal corpus di Gutenberg
                    di NLTK, è stato sviluppato un processo che recupera contenuti testuali e organizza i metadati in
                    un file JSON in modo sistematico. Il corpus viene
                    scaricato attraverso NLTK e i dati archiviati in una
                    directory dedicata. Per ogni libro vengono estratti
                    il testo completo, il titolo e l’autore e salvati in file di testo col
                    nome originale del file Gutenberg. I metadati vengono raccolti in un dizionario Python ed esportati
                    in JSON. Questo approccio è stato esteso ai libri di Open Library, arricchendo con ulteriori informazioni come genere e immagini di copertina, in
                    modo da conservare i dati completi di tutti i libri
                    utilizzati.</p>
            </section>

            <section id="estrazione-frasi">
                <h3>Estrazione Frasi Casuali</h3>
                <p>Una volta ottenuti i file in formato txt corretti
                    e completi, è seguita l’estrazione di frasi casuali dai libri. Poiché i metodi di analisi 
                    semantica non funzionano su segmenti di testo piu lunghi, 
                    si è scelto di selezionare 30 frasi per ciascun libro. 
                    Questa fase ha comportato la creazione di
                    un algoritmo in grado di individuare frasi complete, evitando sequenze di testo troppo brevi o
                    che potessero risultare non pertinenti. Il testo è stato ripulito da caratteri speciali come i simboli
                    di backslash, barra, asterisco e accento circostante;
                    trattando anche i casi delle abbreviazioni per evitare che il programma trattasse erroneamente questi casi come
                    conclusioni di frasi. Per isolare correttamente le
                    frasi, è stato utilizzato un pattern di espressioni regolari che prendeva in considerazione i segni di
                    punteggiatura, implementando poi un filtro che escludeva le frasi troppo brevi, quelle scritte 
                    interamente in maiuscolo o quelle che contenevano più 
                    di una sequenza consecutiva di lettere maiuscole,
                    in quanto ritenute non idonee per il campione.
                    Una volta ottenute le frasi, il programma selezionava casualmente 30 frasi, 
                    assicurandosi che queste
                    rispettassero i criteri di lunghezza e formato. Ogni
                    frase è stata controllata manualmente per correggere eventuali errori e garantire che fosse completamente 
                    leggibile e corretta dal punto di vista
                    sintattico. Il numero di frasi che abbiamo scelto
                    di estrarre rappresenta un compromesso tra la capacità di calcolo dei nostri software e la necessità
                    di garantire la massima affidabilità possibile nei risultati. Tuttavia, suggeriamo di aumentare il 
                    numero di frasi estratte se possibile, poiché un corpus più ampio migliora la probabilità che il campione 
                    sia più rappresentativo. </p>
            </section>

            <section id="parole-frequenti">
                <h3>Parole più Frequenti</h3>
                <p>Nel processo di estrazione delle parole frequenti,
                    il codice sviluppato si occupa di leggere e processare il testo da ciascun file per estrarre un insieme
                    di 50 parole rilevanti in base alla loro frequenza.
                    L’obiettivo è ottenere un elenco delle parole più
                    comuni all’interno di ciascun libro, mentre si effettuano diverse operazioni di pulizia e filtraggio per
                    garantire che solo le parole significative vengano
                    considerate. Il testo, dopo essere stato normalizzato con anche la rimozione della punteggiatura,
                    viene quindi suddiviso in singole parole, applicando un filtro per rimuovere le stopwords. Inoltre, viene impiegata una lista di parole personalizzate da escludere, comprendente termini che, pur
                    essendo frequenti nel contesto del testo, non hanno
                    valore informativo ai fini dell’analisi, come nomi
                    propri e termini troppo specifici. Solo le parole
                    con almeno tre lettere e che non contengano caratteri tipografici particolari come apici o virgolette
                    vengono mantenute. Infine, una volta che tutte le
                    parole rilevanti sono state selezionate, viene calcolata la loro frequenza d’occorrenza nel testo, e
                    viene restituito un elenco ordinato delle parole piu comuni.</p>
            </section>

            <section id="dataset">
                <h3>Dataset</h3>
                <p>Il dataset finale, che verrà utilizzato nel calcolo della similarità e nel sito, si compone delle 30 frasi casuali estratte da ogni libro, delle 50 parole più
                    frequenti e delle informazioni sui libri. Queste informazioni sono state estratte dai metadati di ogni
                    libro e sono state ordinate in un DataFrame creato
                    utilizzando la libreria pandas e in un file JSON,
                    utile nelle dinamiche del sito internet. Questo file
                    include il titolo, l’autore, l’anno di pubblicazione,
                    il genere e il percorso dell’immagine di copertina
                    del libro.</p>
            </section>
        </section>

        <section id="similarita-semantica">
            <h2>Similarità Semantica</h2>
            <p>La seconda fase e iniziata con una ricerca sulla letteratura relativa alla similarità semantica, la quale puo essere definita come la distanza tra i significati semantici di una coppia di parole, frasi, paragrafi o documenti. Dopo una serie di ricerche si
                è deciso di concentrarsi sull’analisi di coppie di parole e frasi. Inoltre, per quanto riguarda la similarità tra frasi, bisogna tenere presente che non esiste un quadro teorico a livello frasale che stabilisca come determinare se due frasi sono semanticamente equivalenti, i criteri sono spesso soggettivi.</p>

            <section id="word-net">
                <h3>Similarità tra Parole con Word Net</h3>
                <p>Per l’analisi delle coppie di parole si è scelto di 
                    utilizzare WordNet1, un enorme database che raggruppa nomi, verbi, aggettivi e avverbi in cognitive synonyms, insiemi di sinonimi che per il
                    parlante nativo indicano un concetto specifico. I
                    synset sono collegati tra loro attraverso relazioni
                    concettuali-semantiche e lessicali, come la relazione di iperonimia, che è stata scelta per 
                    il calcolo dato e risulta essere la relazione codificata con 
                    maggiore frequenza su WordNet. Inoltre, i synset
                    contengono una breve definizione, gloss, la quale
                    comprende anche una o piu frasi brevi con esempi 
                    d’uso. Per il calcolo della similarità si è deciso di 
                    basarsi su questi due aspetti, unendo dunque:</p>
                    <ul>
                        <li>La similarità basata sui percorsi, path-based, 
                            la quale sfrutta la gerarchia concettuale di
                            WordNet, dove valori più alti indicano relazioni semantiche più strette.</li>
                        <li> La similarita basata sulle gloss, la quale utilizza le definizioni e gli esempi dei synset, aggregandoli in un gloss esteso, che viene successivamente preprocessato per rimuovere
                            stopword e punteggiatura, seguito dalla lemmatizzazione. L'idea è molto semplice: se due 
                            parole sono semanticamente simili, allora le
                            glosse estese avranno più parole in comune. 
                            La similarità tra gloss estesi viene valutata utilizzando il coefficiente di similarità di Jaccard, concentrandosi su contenuti comuni tra
                            insiemi.</li>
                    </ul>
                <p>Il punteggio finale le combina, garantendo che
                    vengano sfruttate sia la gerarchia che le glosse presenti in WordNet. Data l’idea di confrontare le
                    parole più frequenti, abbiamo adattato il codice 
                    rendendolo in grado di confrontare liste di parole. Infine, il codice è stato testato con il dataset 
                    WordSim353, ottenendo punteggi di correlazione di Pearson (0.5748) e Spearman (0.6389).
                    Questi punteggi indicano che il metodo utilizzato
                    è abbastanza efficiente, ma non è perfetto; a confronto alcuni sistemi allo stato dell’arte raggiungono un coefficiente di Spearman attorno a 0.8.</p>
                </p>
            </section>

            <section id="similarita-frasi">
                <h3>Similarità tra Frasi</h3>
                <p>Per l’analisi delle frasi intere si è scelto di utilizzare due approcci diversi, uno più tradizionale e il secondo più innovativo.</p>

                <section id="bert">
                    <h4>BERT</h4>
                    <p>Il primo sistema di calcolo della similarità semantica tra frasi è
                        stato implementato utilizzando BERT, in particolare il modello bert-base-uncased,
                        pre-addestrato su un ampio corpus di testi in
                        inglese, della libreria Hugging Face Transformers. Il tokenizzatore di BERT segmenta le frasi
                        in token e li converte in rappresentazioni numeriche compatibili con il modello. Inoltre, il tokenizzatore gestisce automaticamente la lunghezza
                        delle frasi applicando il padding per uniformarle
                        o la troncatura se superano il limite di 128 token. Dopo la tokenizzazione si attua l’estrazione
                        di embedding: il modello genera vettori semantici per ogni frase calcolando la media dei vettori degli ultimi strati. Si è scelto di utilizzare
                        with torch.no_grad(): per ridurre l’uso
                        della memoria e velocizzare il calcolo. Infine, la
                        similarità tra due frasi è determinata confrontando
                        i loro rispettivi embedding tramite la similarità
                        coseno. Il codice è stato poi testato con il dataset 
                        GLUE di STS-benchmark, ottenendo una correlazione di Pearson: 0.87 e una correlazione di
                        spearman: 0.86; sono punteggi buoni, anche se lo
                        stato dell’arte attuale si aggira attorno allo 0.92.</p>
                </section>

                <section id="framenet">
                    <h4>FrameNet</h4>
                    <p>L’ultimo approccio che abbiamo sviluppato si
                        basa vagamente sul lavoro di basile-etal-2018-measuring. Il nostro approccio propone un
                        metodo per calcolare la similarità semantica utilizzando le informazioni combinate di FrameNet e
                        WordNet, unendo la struttura semantica dai
                        tipi di frame di FrameNet alla ricchezza del
                        database di WordNet. La semantica dei frame
                        è stata sviluppata da Fillmore, mettendo in relazione significato linguistico e conoscenza enciclopedica del mondo. In particolare, un frame è
                        definitivo come una struttura concettuale simile a
                        una scena che descrive un particolare tipo di situazione, oggetto o evento, insieme ai partecipanti
                        e agli elementi necessari per quel frame. Le parole che evocano un frame sono definite come lexical units (LUs), mentre i frame elements sono le
                        componenti chiave che definiscono e contestualizzano il significato di un evento o di un concetto
                        descritto dal frame, hanno la funzione di un ruolo
                        semantico. Basandosi su questi aspetti, la nostra
                        metodologia si pone su tre componenti principali:
                    <ul>
                        <li>Relatedness tra Frame Types: i frame semantici di FrameNet sono identificati tramite
                            le Lexical Units (LUs) associate alle parole
                            di ciascuna frase. La similarità tra i tipi di
                            frame è calcolata come la media delle similarità tra tutte le coppie di frame, utilizzando 
                            una soglia per filtrare similarità irrilevanti.</li>
                        <li>Relatedness tra Frame Elements: i frame elements di ciascun frame sono analizzati per
                            valutare la loro affinità tramite WordNet.</li>
                        <li>Relatedness complessiva: la misura complessiva combina i due componenti sopra menzionati utilizzando una pesatura dinamica.
                            Ogni frase viene analizzata tramite un processo di tokenizzazione e POS-tagging, per
                            poi estrarre le Lexical Units, frame types e
                            elementi semantici da ciascun token.
                            </li>
                    </ul>
                    <p>Per velocizzare il calcolo è stata inclusa una parte
                    di precaricamento e parallelizzazione, evitando
                    calcoli ripetitivi e migliorando l’efficienza. Il
                    codice è stato testato con il dataset GLUE di STS- `
                    benchmark su google colab, ottenendo una correlazione di Pearson pari a 0.41 e una correlazione
                    di Spearman pari a 0.39; sono punteggi moderati,
                    mostrando che si possono ottenere dei miglioramenti, magari considerando anche le relazioni gerarchiche presenti in FrameNet.</p>
                </section>
            </section>
        </section>

        <section id="confronti-risultati">
            <h2>Confronti e Risultati</h2>
            <p>Nel processo di analisi e confronto tra i libri, abbiamo utilizzato i tre diversi modelli di similarità
                con il fine di generare dei file JSON che fossero
                facilmente utilizzabili dal sito per visualizzare e
                interagire con i risultati. L’applicazione del modello di confronto basato sulle parole ha richiesto
                un codice relativamente semplice, che produce un
                file JSON strutturato in cui, per ogni libro, vengono riportati i libri con cui è stato confrontato, 
                insieme ai rispettivi punteggi di similarità. Questo 
                modello, confronta direttamente le liste di parole
                frequenti di due libri, fornendo un unico punteggio
                finale, rappresentativo della similarità complessiva 
                tra i due. I modelli basati su BERT e Framenet per il confronto tra frasi
                hanno richiesto un approccio piu complesso poiché
                forniscono un punteggio di similarità per ogni coppia di frasi e ogni libro presenta 30 frasi. Questo
                significa che il risultato del confronto tra due libri
                non viene rappresentato da un singolo valore, ma
                da 900 (30 x 30 frasi = 900 risultati). Inoltre,
                questo tipo di analisi può generare risultati con un 
                ampio range di valori, a causa della casualità nel 
                campionamento delle frasi per il confronto. Ad
                esempio, una singola frase molto simile a un’altra
                potrebbe generare un punteggio elevato, ma non
                essere rappresentativa della relazione complessiva
                tra i due libri. Per migliorare la validità dei risultati finali, è stata applicata una strategia di calcolo 
                della media ponderata. Questa media tiene conto
                del numero di risultati sopra una certa soglia (nel
                nostro caso, una soglia di similarità pari a 0.15 per BERT e 0.48 per FrameNet), 
                in modo che la presenza di un numero maggiore
                di frasi simili valga più dei punteggi individuali di 
                singole coppie di frasi. In altre parole, i punteggi
                finali non dipendono solo dalla similarità di una 
                coppia di frasi, ma anche dalla frequenza con cui
                le frasi di due libri risultano simili tra loro. Questo
                approccio aiuta a ridurre l’impatto della casualità
                e a ottenere una valutazione piu affidabile e coerente della similarità tra i libri. Una volta calcolata la media ponderata per ciascun confronto di
                frasi, anche questi risultati vengono salvati in un
                file JSON, strutturato in modo da includere, per
                ogni libro, le informazioni relative ai confronti con
                gli altri libri, accompagnate dai punteggi di similarità medi ponderati.</p>
        </section>

        <section id="sito-web">
            <h2>Sito Web</h2>
            <p>Il sito web è stato realizzato con l’obiettivo di 
                consentire agli utenti di individuare libri semanticamente simili tra di loro. La homepage permette di effettuare una ricerca diretta utilizzando
                una barra dedicata, oppure di esplorare il dataset
                dei libri analizzati, da cui è possibile accedere 
                a informazioni dettagliate sui singoli libri e, selezionandone uno, scoprire i tre titoli piu simili. 
                La similarità tra i libri è calcolata utilizzando un metodo che combina tre modelli distinti, ciascuno con un peso specifico. Il modello basato sulle parole ha un peso maggiore, seguito da quello basato su BERT, mentre il modello
                basato su FrameNet, risultato meno affidabile, ha
                un’influenza minore sul calcolo finale. Per ogni
                libro simile viene mostrata una percentuale complessiva di similarità,
                offrendo un’indicazione sintetica ma significativa del grado di affinità. In 
                aggiunta, è possibile accedere ai dettagli avanzati 
                per confrontare i risultati forniti dai singoli modelli, evidenziando eventuali differenze nei calcoli
                e nelle interpretazioni.
                Il sito include anche una sezione informativa dedicata al progetto, in cui sono spiegati gli obiettivi
                e le metodologie utilizzate. Dal punto di vista tecnico, il sito è stato sviluppato utilizzando HTML 
                per la struttura, CSS per lo stile
                e JavaScript per rendere interattiva la navigazione
                e per gestire dinamicamente i dati caricati.</p>
        </section>

        <section id="conclusioni">
            <h2>Conclusioni</h2>
            <p>SemanticShelf rappresenta una dimostrazione tangibile di 
                come l’elaborazione del linguaggio naturale e le tecnologie semantiche possano ridefinire 
                il modo in cui esploriamo e scopriamo nuove letture. Attraverso un approccio basato sull’analisi semantica avanzata e l’integrazione 
                di modelli innovativi come quelli basati su WordNet, BERT e FrameNet, il progetto si è proposto di eludere i tradizionali sistemi di 
                raccomandazione. La combinazione tra tecniche di web scraping, elaborazione dei dati e sviluppo di algoritmi 
                personalizzati ha permesso di creare una piattaforma che, con un ampliamento del corpus di partenza e un'adeguata capacità di calcolo
                potrebbe essere in grado di offrire raccomandazioni personalizzate e 
                accurate, valorizzando i contenuti dei libri piuttosto che affidarsi solo a criteri esterni come genere o autore.
                Il sito web di SemanticShelf rende accessibile questa tecnologia in modo intuitivo, 
                favorendo un’interazione diretta con i risultati delle analisi e dimostrando come queste tecnologie possano rivelarsi adatte alle 
                esigenze degli utenti. Sebbene il sistema abbia margini di miglioramento e alcuni limiti dovuti alla pesantezza di alcuni modelli di simalarità, i risultati raggiunti finora testimoniano il potenziale del progetto.
                In un’epoca in cui l’accesso alla conoscenza e alla cultura 
                è cruciale, SemanticShelf offre un esempio di come la tecnologia possa essere 
                utilizzata per esplorare nuove connessioni tra i libri, stimolando l’apprendimento, 
                la curiosità e la scoperta. Questo lavoro apre la strada a ulteriori sviluppi nel campo 
                della raccomandazione semantica, con possibilità di espansione verso altri tipi di contenuti e applicazioni.</p>
        </section>

        <section id="references">
            <h2>References</h2>
            <p>Beautiful Soup documentation, https://www.
                crummy.com/software/BeautifulSoup/
                bs4/doc<br /><br />
                spaCy documentation, https://spacy.io/
                usage/linguistic-features<br /><br />
                Matplotlib documentation, https://
                matplotlib.org/stable/users/index.
                html<br /><br />
                Flask documentation, https://flask.
                palletsprojects.com/en/3.0.x<br /><br />
                Bootrap documentation, https://
                getbootstrap.com/docs/5.3/</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Progetto SemanticShelf</p>
    </footer>
</body>
</html>
